{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audiobook .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRlp3LGoRCDA2Dw2Hvjjb4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirat89/Complete-Python-3-Bootcamp/blob/master/Audiobook_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chGJdWbgaZ49"
      },
      "source": [
        "# You are given data from an Audiobook App. Logically, it relates to the audio versions of books ONLY. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
        "\n",
        "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertising to him/her. If we can focus our efforts SOLELY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIKtu6jVNc3N"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOzPPEFkZxw-",
        "outputId": "2f026a6f-c432-4fd8-a104-56f0cb180334"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mHCeS7OZx2b"
      },
      "source": [
        "raw_csv_data=np.loadtxt('/content/drive/MyDrive/Data Science /Audiobooks_data.csv',delimiter=',')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WGYdiYxZyEh"
      },
      "source": [
        "# The inputs are all columns in the csv, except for the first one [:,0]\n",
        "# (which is just the arbitrary customer IDs that bear no useful information),\n",
        "# and the last one [:,-1] (which is our targets)\n",
        "unscaled_input_all=raw_csv_data[:,1:-1]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDBtHH6ecBff"
      },
      "source": [
        "# The targets are in the last column.\n",
        "targets=raw_csv_data[:,-1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AslLIQ4Po45"
      },
      "source": [
        "shuffle_indices=np.arange(unscaled_input_all.shape[0])\n",
        "np.random.shuffle(shuffle_indices)\n",
        "# Use the shuffled indices to shuffle the inputs and targets.\n",
        "p_shuffled_inputs = unscaled_input_all[shuffle_indices]\n",
        "p_shuffled_targets = targets[shuffle_indices]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqAEVZ_WcghA"
      },
      "source": [
        "since in the given dataset most of the customer didn't buy back, so targets are imbalanced. Their's a chance machine could learn that buying back is not possiblity.\n",
        "so to avoid this, we will **balance the dataset**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWINDm5JcYv6"
      },
      "source": [
        "# Count how many targets are 1 (meaning that the customer did convert)\n",
        "num_one_targets=int(np.sum(p_shuffled_targets))\n",
        "#setting a counter for coustomers that didn't convert(0)\n",
        "zero_targets_counter = 0\n",
        "# We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n",
        "# Declare a variable that will do that:\n",
        "indices_to_remove = []\n",
        "for i in range(p_shuffled_targets.shape[0]):\n",
        "  if p_shuffled_targets[i]==0:\n",
        "   zero_targets_counter+=1\n",
        "   if zero_targets_counter>num_one_targets:\n",
        "     indices_to_remove.append(i)\n",
        "\n",
        "\n",
        "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
        "# We delete all indices that we marked \"to remove\" in the loop above.\n",
        "balanced_inputs = np.delete(p_shuffled_inputs, indices_to_remove, axis=0)\n",
        "balanced_targets = np.delete(p_shuffled_targets, indices_to_remove, axis=0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7VMDXpZm1d2",
        "outputId": "79b93990-957a-41a7-9a01-7d268a53da94"
      },
      "source": [
        "unique, counts = np.unique(balanced_targets, return_counts=True)\n",
        "\n",
        "print (np.asarray((unique, counts)).T)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.000e+00 2.237e+03]\n",
            " [1.000e+00 2.237e+03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pQ8ipDytYZf"
      },
      "source": [
        "shuffle_indices=np.arange(balanced_inputs.shape[0])\n",
        "np.random.shuffle(shuffle_indices)\n",
        "# Use the shuffled indices to shuffle the inputs and targets.\n",
        "shuffled_inputs = unscaled_input_all[shuffle_indices]\n",
        "shuffled_targets = targets[shuffle_indices]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k2uOjgKdFFa"
      },
      "source": [
        "scaled_shuffled_inputs = preprocessing.scale(shuffled_inputs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUzXz7wM0jZ3",
        "outputId": "bad5dd5e-ce34-4f36-e036-9d6ec7260552"
      },
      "source": [
        "samples_count = scaled_shuffled_inputs.shape[0]\n",
        "\n",
        "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
        "# Naturally, the numbers are integers.\n",
        "train_samples_count = int(0.8 * samples_count)\n",
        "validation_samples_count = int(0.1 * samples_count)\n",
        "\n",
        "# The 'test' dataset contains all remaining data.\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "# Create variables that record the inputs and targets for training\n",
        "# In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
        "train_inputs = scaled_shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "\n",
        "# Create variables that record the inputs and targets for validation.\n",
        "# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
        "validation_inputs = scaled_shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
        "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
        "\n",
        "# Create variables that record the inputs and targets for test.\n",
        "# They are everything that is remaining.\n",
        "test_inputs = scaled_shuffled_inputs[train_samples_count+validation_samples_count:]\n",
        "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
        "\n",
        "# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were \n",
        "# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, \n",
        "# you will get different values, as each time they are shuffled randomly.\n",
        "# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n",
        "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n",
        "\n",
        "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "718.0 3579 0.20061469684269348\n",
            "84.0 447 0.18791946308724833\n",
            "85.0 448 0.18973214285714285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHGGRBxnu5oK",
        "outputId": "e2595fe5-8ce4-4f1f-dca8-e023927f958e"
      },
      "source": [
        "unique, counts = np.unique(test_targets, return_counts=True)\n",
        "\n",
        "print (np.asarray((unique, counts)).T)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0. 363.]\n",
            " [  1.  85.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxiBu_Yo3Mcw"
      },
      "source": [
        "np.savez('/content/drive/MyDrive/Data Science /Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
        "np.savez('/content/drive/MyDrive/Data Science /Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
        "np.savez('/content/drive/MyDrive/Data Science /Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymNHGgCIAClk"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8INwFKu0AC2R"
      },
      "source": [
        "npz=np.load('/content/drive/MyDrive/Data Science /Audiobooks_data_train.npz')\n",
        "\n",
        "train_inputs=npz['inputs'].astype(np.float)\n",
        "train_targets=npz['targets'].astype(np.int)\n",
        "\n",
        "npz=np.load('/content/drive/MyDrive/Data Science /Audiobooks_data_validation.npz')\n",
        "\n",
        "validation_inputs=npz['inputs'].astype(np.float)\n",
        "validation_targets=npz['targets'].astype(np.int)\n",
        "\n",
        "npz=np.load('/content/drive/MyDrive/Data Science /Audiobooks_data_test.npz')\n",
        "\n",
        "test_inputs=npz['inputs'].astype(np.float)\n",
        "test_targets=npz['targets'].astype(np.int)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9iU1_ZUSIWP"
      },
      "source": [
        "input_size=10\n",
        "output_size=2\n",
        "hidden_layer_size=100\n",
        "\n",
        "model=tf.keras.Sequential([\n",
        "                           tf.keras.layers.Dense(units=input_size,activation='relu'),\n",
        "                           tf.keras.layers.Dense(units=hidden_layer_size,activation='relu'),\n",
        "                           tf.keras.layers.Dense(units=hidden_layer_size,activation='relu'),\n",
        "                           tf.keras.layers.Dense(units=output_size,activation='softmax')\n",
        "                           ])\n",
        "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BNVIarP7Mar",
        "outputId": "73ce9bc0-b688-4ec1-8bc2-3bffb6308d90"
      },
      "source": [
        "batch_size=100\n",
        "epochs=100\n",
        "early_stopping=tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "epoch_hist=model.fit(train_inputs,train_targets,batch_size=batch_size,epochs=epochs,callbacks=[early_stopping],validation_data=(validation_inputs,validation_targets),\n",
        "verbose=2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "36/36 - 1s - loss: 0.5582 - accuracy: 0.7737 - val_loss: 0.4437 - val_accuracy: 0.8389\n",
            "Epoch 2/100\n",
            "36/36 - 0s - loss: 0.3891 - accuracy: 0.8514 - val_loss: 0.3466 - val_accuracy: 0.8635\n",
            "Epoch 3/100\n",
            "36/36 - 0s - loss: 0.3392 - accuracy: 0.8676 - val_loss: 0.3279 - val_accuracy: 0.8591\n",
            "Epoch 4/100\n",
            "36/36 - 0s - loss: 0.3218 - accuracy: 0.8681 - val_loss: 0.3143 - val_accuracy: 0.8680\n",
            "Epoch 5/100\n",
            "36/36 - 0s - loss: 0.3123 - accuracy: 0.8731 - val_loss: 0.3258 - val_accuracy: 0.8702\n",
            "Epoch 6/100\n",
            "36/36 - 0s - loss: 0.3038 - accuracy: 0.8737 - val_loss: 0.3018 - val_accuracy: 0.8792\n",
            "Epoch 7/100\n",
            "36/36 - 0s - loss: 0.2945 - accuracy: 0.8801 - val_loss: 0.2985 - val_accuracy: 0.8770\n",
            "Epoch 8/100\n",
            "36/36 - 0s - loss: 0.2914 - accuracy: 0.8815 - val_loss: 0.3032 - val_accuracy: 0.8770\n",
            "Epoch 9/100\n",
            "36/36 - 0s - loss: 0.2895 - accuracy: 0.8779 - val_loss: 0.2961 - val_accuracy: 0.8814\n",
            "Epoch 10/100\n",
            "36/36 - 0s - loss: 0.2838 - accuracy: 0.8838 - val_loss: 0.2943 - val_accuracy: 0.8770\n",
            "Epoch 11/100\n",
            "36/36 - 0s - loss: 0.2798 - accuracy: 0.8854 - val_loss: 0.2920 - val_accuracy: 0.8725\n",
            "Epoch 12/100\n",
            "36/36 - 0s - loss: 0.2778 - accuracy: 0.8826 - val_loss: 0.3026 - val_accuracy: 0.8747\n",
            "Epoch 13/100\n",
            "36/36 - 0s - loss: 0.2772 - accuracy: 0.8846 - val_loss: 0.2814 - val_accuracy: 0.8792\n",
            "Epoch 14/100\n",
            "36/36 - 0s - loss: 0.2740 - accuracy: 0.8846 - val_loss: 0.2891 - val_accuracy: 0.8792\n",
            "Epoch 15/100\n",
            "36/36 - 0s - loss: 0.2717 - accuracy: 0.8871 - val_loss: 0.2815 - val_accuracy: 0.8814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Siu4Bco7NOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e25eab-caa3-4948-a18d-6e60434d0246"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2162 - accuracy: 0.9241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XNAFlUDXX_v",
        "outputId": "10edba47-fe1d-45c5-c93b-733c460e7f41"
      },
      "source": [
        "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test loss: 0.22. Test accuracy: 92.41%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}