{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audiobook .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxE7XGQJ5wiJ5GZCvSHghN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirat89/Complete-Python-3-Bootcamp/blob/master/Audiobook_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chGJdWbgaZ49"
      },
      "source": [
        "# You are given data from an Audiobook App. Logically, it relates to the audio versions of books ONLY. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
        "\n",
        "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertising to him/her. If we can focus our efforts SOLELY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIKtu6jVNc3N"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOzPPEFkZxw-",
        "outputId": "0deb61c2-4385-4735-c18f-7afac57e0562"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mHCeS7OZx2b"
      },
      "source": [
        "raw_csv_data=np.loadtxt('/content/drive/MyDrive/Data Science /Audiobooks_data.csv',delimiter=',')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WGYdiYxZyEh"
      },
      "source": [
        "# The inputs are all columns in the csv, except for the first one [:,0]\n",
        "# (which is just the arbitrary customer IDs that bear no useful information),\n",
        "# and the last one [:,-1] (which is our targets)\n",
        "unscaled_input_all=raw_csv_data[:,1:-1]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDBtHH6ecBff"
      },
      "source": [
        "# The targets are in the last column.\n",
        "targets=raw_csv_data[:,-1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AslLIQ4Po45"
      },
      "source": [
        "shuffle_indices=np.arange(unscaled_input_all.shape[0])\n",
        "np.random.shuffle(shuffle_indices)\n",
        "# Use the shuffled indices to shuffle the inputs and targets.\n",
        "p_shuffled_inputs = unscaled_input_all[shuffle_indices]\n",
        "p_shuffled_targets = targets[shuffle_indices]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqAEVZ_WcghA"
      },
      "source": [
        "since in the given dataset most of the customer didn't buy back, so targets are imbalanced. Their's a chance machine could learn that buying back is not possiblity.\n",
        "so to avoid this, we will **balance the dataset**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWINDm5JcYv6"
      },
      "source": [
        "# Count how many targets are 1 (meaning that the customer did convert)\n",
        "num_one_targets=int(np.sum(p_shuffled_targets))\n",
        "#setting a counter for coustomers that didn't convert(0)\n",
        "zero_targets_counter = 0\n",
        "# We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n",
        "# Declare a variable that will do that:\n",
        "indices_to_remove = []\n",
        "for i in range(p_shuffled_targets.shape[0]):\n",
        "  if p_shuffled_targets[i]==0:\n",
        "   zero_targets_counter+=1\n",
        "   if zero_targets_counter>num_one_targets:\n",
        "     indices_to_remove.append(i)\n",
        "\n",
        "\n",
        "# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
        "# We delete all indices that we marked \"to remove\" in the loop above.\n",
        "balanced_inputs = np.delete(p_shuffled_inputs, indices_to_remove, axis=0)\n",
        "balanced_targets = np.delete(p_shuffled_targets, indices_to_remove, axis=0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7VMDXpZm1d2",
        "outputId": "668db5bd-9092-4a95-fe19-30abb0d1cc7a"
      },
      "source": [
        "unique, counts = np.unique(balanced_targets, return_counts=True)\n",
        "\n",
        "print (np.asarray((unique, counts)).T)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.000e+00 2.237e+03]\n",
            " [1.000e+00 2.237e+03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pQ8ipDytYZf"
      },
      "source": [
        "shuffle_indices=np.arange(balanced_inputs.shape[0])\n",
        "np.random.shuffle(shuffle_indices)\n",
        "# Use the shuffled indices to shuffle the inputs and targets.\n",
        "shuffled_inputs = unscaled_input_all[shuffle_indices]\n",
        "shuffled_targets = targets[shuffle_indices]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k2uOjgKdFFa"
      },
      "source": [
        "scaled_shuffled_inputs = preprocessing.scale(shuffled_inputs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUzXz7wM0jZ3",
        "outputId": "7ecf7460-45e4-41de-ddc7-e877cc912ae5"
      },
      "source": [
        "samples_count = scaled_shuffled_inputs.shape[0]\n",
        "\n",
        "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
        "# Naturally, the numbers are integers.\n",
        "train_samples_count = int(0.8 * samples_count)\n",
        "validation_samples_count = int(0.1 * samples_count)\n",
        "\n",
        "# The 'test' dataset contains all remaining data.\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "# Create variables that record the inputs and targets for training\n",
        "# In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
        "train_inputs = scaled_shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "\n",
        "# Create variables that record the inputs and targets for validation.\n",
        "# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
        "validation_inputs = scaled_shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
        "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
        "\n",
        "# Create variables that record the inputs and targets for test.\n",
        "# They are everything that is remaining.\n",
        "test_inputs = scaled_shuffled_inputs[train_samples_count+validation_samples_count:]\n",
        "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
        "\n",
        "# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were \n",
        "# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, \n",
        "# you will get different values, as each time they are shuffled randomly.\n",
        "# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n",
        "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n",
        "\n",
        "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "717.0 3579 0.20033528918692373\n",
            "80.0 447 0.1789709172259508\n",
            "90.0 448 0.20089285714285715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHGGRBxnu5oK",
        "outputId": "39e7f24c-08fc-4e7c-dc1f-c844124b05bc"
      },
      "source": [
        "unique, counts = np.unique(test_targets, return_counts=True)\n",
        "\n",
        "print (np.asarray((unique, counts)).T)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0. 358.]\n",
            " [  1.  90.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxiBu_Yo3Mcw"
      },
      "source": [
        "np.savez('/content/drive/MyDrive/Data Science /Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
        "np.savez('/content/drive/MyDrive/Data Science /Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
        "np.savez('/content/drive/MyDrive/Data Science /Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymNHGgCIAClk"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8INwFKu0AC2R"
      },
      "source": [
        "npz=np.load('/content/drive/MyDrive/Data Science /Audiobooks_data_train.npz')\n",
        "\n",
        "train_inputs=npz['inputs'].astype(np.float)\n",
        "train_targets=npz['targets'].astype(np.int)\n",
        "\n",
        "npz=np.load('/content/drive/MyDrive/Data Science /Audiobooks_data_validation.npz')\n",
        "\n",
        "validation_inputs=npz['inputs'].astype(np.float)\n",
        "validation_targets=npz['targets'].astype(np.int)\n",
        "\n",
        "npz=np.load('/content/drive/MyDrive/Data Science /Audiobooks_data_test.npz')\n",
        "\n",
        "test_inputs=npz['inputs'].astype(np.float)\n",
        "test_targets=npz['targets'].astype(np.int)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9iU1_ZUSIWP"
      },
      "source": [
        "input_size=10\n",
        "output_size=2\n",
        "hidden_layer_size=100\n",
        "\n",
        "model=tf.keras.Sequential([\n",
        "                           tf.keras.layers.Dense(units=input_size,activation='relu'),\n",
        "                           tf.keras.layers.Dense(units=hidden_layer_size,activation='relu'),\n",
        "                           tf.keras.layers.Dense(units=hidden_layer_size,activation='relu'),\n",
        "                           tf.keras.layers.Dense(units=output_size,activation='softmax')\n",
        "                           ])\n",
        "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BNVIarP7Mar",
        "outputId": "1d78ec51-0a21-4f7d-b2f6-ebee7cf812e0"
      },
      "source": [
        "batch_size=100\n",
        "epochs=100\n",
        "early_stopping=tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "epoch_hist=model.fit(train_inputs,train_targets,batch_size=batch_size,epochs=epochs,callbacks=[early_stopping],validation_data=(validation_inputs,validation_targets),\n",
        "verbose=2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "36/36 - 1s - loss: 0.4956 - accuracy: 0.8011 - val_loss: 0.4051 - val_accuracy: 0.8523\n",
            "Epoch 2/100\n",
            "36/36 - 0s - loss: 0.3942 - accuracy: 0.8500 - val_loss: 0.3508 - val_accuracy: 0.8680\n",
            "Epoch 3/100\n",
            "36/36 - 0s - loss: 0.3468 - accuracy: 0.8653 - val_loss: 0.3193 - val_accuracy: 0.8747\n",
            "Epoch 4/100\n",
            "36/36 - 0s - loss: 0.3235 - accuracy: 0.8729 - val_loss: 0.2977 - val_accuracy: 0.8837\n",
            "Epoch 5/100\n",
            "36/36 - 0s - loss: 0.3114 - accuracy: 0.8793 - val_loss: 0.2871 - val_accuracy: 0.8814\n",
            "Epoch 6/100\n",
            "36/36 - 0s - loss: 0.2969 - accuracy: 0.8829 - val_loss: 0.2955 - val_accuracy: 0.8904\n",
            "Epoch 7/100\n",
            "36/36 - 0s - loss: 0.2868 - accuracy: 0.8826 - val_loss: 0.2835 - val_accuracy: 0.8926\n",
            "Epoch 8/100\n",
            "36/36 - 0s - loss: 0.2806 - accuracy: 0.8852 - val_loss: 0.2766 - val_accuracy: 0.8926\n",
            "Epoch 9/100\n",
            "36/36 - 0s - loss: 0.2773 - accuracy: 0.8868 - val_loss: 0.2759 - val_accuracy: 0.8904\n",
            "Epoch 10/100\n",
            "36/36 - 0s - loss: 0.2754 - accuracy: 0.8885 - val_loss: 0.2751 - val_accuracy: 0.8770\n",
            "Epoch 11/100\n",
            "36/36 - 0s - loss: 0.2717 - accuracy: 0.8888 - val_loss: 0.2580 - val_accuracy: 0.9016\n",
            "Epoch 12/100\n",
            "36/36 - 0s - loss: 0.2676 - accuracy: 0.8894 - val_loss: 0.2648 - val_accuracy: 0.8971\n",
            "Epoch 13/100\n",
            "36/36 - 0s - loss: 0.2673 - accuracy: 0.8899 - val_loss: 0.2521 - val_accuracy: 0.8993\n",
            "Epoch 14/100\n",
            "36/36 - 0s - loss: 0.2639 - accuracy: 0.8921 - val_loss: 0.2833 - val_accuracy: 0.8859\n",
            "Epoch 15/100\n",
            "36/36 - 0s - loss: 0.2662 - accuracy: 0.8899 - val_loss: 0.2559 - val_accuracy: 0.8993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Siu4Bco7NOm"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}